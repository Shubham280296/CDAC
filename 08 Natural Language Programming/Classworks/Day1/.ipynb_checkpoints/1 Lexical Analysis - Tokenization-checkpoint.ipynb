{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f3a2c2",
   "metadata": {},
   "source": [
    "## Library installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dd0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0211611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') ## tokenization\n",
    "nltk.download('stopwords') ## stopwords removal\n",
    "nltk.download('averaged_perceptron_tagger') ## part of speech (speech) tagging\n",
    "nltk.download('wordnet') ## wordnet database and lemmatization\n",
    "nltk.download('omw-1.4') ## stemmin\n",
    "nltk.download('indian') ## Indian language pos tagging\n",
    "nltk.download('maxent_ne_chunker') ## chunkingh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696b63b",
   "metadata": {},
   "source": [
    "## Sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf94c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the average of ages mentioned in the above sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff92ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "sent = 'They told that their ages are 25   27 and 31 respectively.'\n",
    "ages = []\n",
    "\n",
    "for i in sent.split(\" \"):\n",
    "    if i.isdigit():\n",
    "        ages.append(int(i))\n",
    "        \n",
    "print(sum(ages)/len(ages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54b0ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "ages = [int(word) for word in sent.split() if word.isdigit()]\n",
    "print(sum(ages)/len(ages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b750d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8caf0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45af25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43d049db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the functions for tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e66e07ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eea05e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eae45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# Find the percentage of punctuation symbols present in it\n",
    "\n",
    "count =0\n",
    "\n",
    "for i in word_tokenize(sent):\n",
    "    if i.isalnum():\n",
    "        count +=1\n",
    "        \n",
    "print((1-(count/len(word_tokenize(sent))))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bf3ba9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_count = len([word for word in word_tokenize(sent) if not word.isalnum()])\n",
    "punct_count/len(word_tokenize(sent))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d29f05",
   "metadata": {},
   "source": [
    "## Ascii code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fe2ed43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('#') # get ascii value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7aa81035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('#')  ## min size of object is 50 in ascii though only 8 are used for one char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6cd3a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('abcdfgh') ## 8*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f216c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(75) ## returns character associated with number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466aa906",
   "metadata": {},
   "source": [
    "## Universal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "424423fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µ\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0935' ## universal code\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbee5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µà¥€\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0935\\u0940'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3994bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¶'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(2358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "813eb546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µ'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x935) # hexadecimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a01c5314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('à¤µ') ## min size of object is 76 in universal though only 8 are used for one char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf33facb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤¶à¥à¤­à¤®', 'à¤¶à¤¹à¤¾']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'à¤¶à¥à¤­à¤® à¤¶à¤¹à¤¾'\n",
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a4327b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('à¤¶')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3d797b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µà¥à¤­à¤® à¤µà¤¹à¤¾'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('à¤¶', 'à¤µ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23ae9716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('à¤®')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8b85701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8aadccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¥'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f75df",
   "metadata": {},
   "source": [
    "## Text in regional languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2d959",
   "metadata": {},
   "source": [
    "mytext = 'à¥ªà¥® à¤•à¤¿. à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡. à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡. à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾. à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡. à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡. à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€ Archived 2020-03-18 at the Wayback Machine. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€ à¤¨à¤¿à¤µà¤¡ à¤•à¥‡à¤²à¥€. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾ à¤¤à¥€à¤¨ à¤®à¤¾à¤šà¥à¤¯à¤¾ à¤µ à¤à¤• à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤–à¥‚à¤ª à¤‰à¤‚à¤š à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¥€ à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¥§à¥©à¥¯à¥ª à¤®à¥€à¤Ÿà¤° à¤†à¤¹à¥‡. à¤¦à¥à¤°à¥à¤—à¤°à¤¾à¤œ à¤°à¤¾à¤œà¤—à¤¡ à¤¤à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤•à¤¾à¤‚à¤•à¥à¤·à¥‡à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¤¦à¤¾à¤–à¤µà¤¤à¥‹, à¤¤à¤° à¤•à¤¿à¤²à¥à¤²à¥‡ à¤°à¤¾à¤¯à¤—à¤¡ à¤¹à¤¾ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤•à¤°à¥à¤¤à¥ƒà¤¤à¥à¤µà¤¾à¤šà¤¾ à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¦à¤¾à¤–à¤µà¤¤à¥‹. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤§à¥à¤¯à¤µà¤°à¥à¤¤à¥€ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤‰à¤‚à¤š à¤¡à¥‹à¤‚à¤—à¤° à¤¤à¤¾à¤¸à¥‚à¤¨ à¤¤à¤¯à¤¾à¤° à¤•à¥‡à¤²à¥‡à¤²à¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤®à¥à¤¹à¤£à¤œà¥‡ à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤¨à¥‡ à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤µà¤° à¤•à¥‡à¤²à¥‡à¤²à¥€ à¤¸à¥à¤µà¤¾à¤°à¥€ à¤¹à¥‹à¤¯.'\n",
    "mytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac81d184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¥ªà¥®',\n",
       " 'à¤•à¤¿',\n",
       " '.',\n",
       " 'à¤®à¥€',\n",
       " '.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤²',\n",
       " 'à¤µà¥‡à¤²à¥à¤¹à¥‡',\n",
       " 'à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤',\n",
       " 'à¤µ',\n",
       " 'à¤­à¥‹à¤°',\n",
       " 'à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾',\n",
       " 'à¥¨à¥ª',\n",
       " 'à¤•à¤¿.à¤®à¥€',\n",
       " '.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°',\n",
       " 'à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤—à¥à¤‚à¤œà¤µà¤£à¥€',\n",
       " 'à¤¯à¤¾',\n",
       " 'à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤',\n",
       " 'à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾',\n",
       " 'à¤¡à¥‹à¤‚à¤—à¤°',\n",
       " 'à¤‰à¤­à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤®à¤¾à¤µà¤³',\n",
       " 'à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡',\n",
       " 'à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°',\n",
       " 'à¤¸à¤¾à¤§à¥à¤¯',\n",
       " 'à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾',\n",
       " 'à¤¹à¥‡',\n",
       " 'à¤¦à¥‹à¤¨à¥à¤¹à¥€',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥‡',\n",
       " 'à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤ à¤¿à¤•à¤¾à¤£à¥€',\n",
       " 'à¤¹à¥‹à¤¤à¥‡',\n",
       " '.',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾',\n",
       " 'Archived',\n",
       " '2020-09-20',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡',\n",
       " 'à¤²à¤¹à¤¾à¤¨',\n",
       " 'à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤•à¥€à¤¯',\n",
       " 'à¤•à¥‡à¤‚à¤¦à¥à¤°',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤¹à¤¾',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤¸à¥‹à¤¯à¥€à¤šà¤¾',\n",
       " 'à¤¨à¤µà¥à¤¹à¤¤à¤¾',\n",
       " '.',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤¦à¥à¤°à¥à¤—à¤®',\n",
       " 'à¤…à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤¬à¤°à¤¾à¤š',\n",
       " 'à¤®à¥‹à¤ à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤¯',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡',\n",
       " 'à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€',\n",
       " 'à¤¬à¤¾à¤œà¥‚à¤¨à¥‡',\n",
       " 'à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾',\n",
       " 'à¤à¤–à¤¾à¤¦à¥€',\n",
       " 'à¤Ÿà¥‡à¤•à¤¡à¥€',\n",
       " 'à¤•à¤¿à¤‚à¤µà¤¾',\n",
       " 'à¤¨à¤¦à¥€',\n",
       " 'à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š',\n",
       " 'à¤²à¤¾à¤—à¤¤à¥‡',\n",
       " '.',\n",
       " 'à¤à¤µà¤¢à¥€',\n",
       " 'à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾',\n",
       " 'à¤¹à¥‹à¤¤à¥€',\n",
       " ',',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤†à¤ªà¤²à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤•à¥€à¤¯',\n",
       " 'à¤•à¥‡à¤‚à¤¦à¥à¤°',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€',\n",
       " 'Archived',\n",
       " '2020-03-18',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€',\n",
       " 'à¤¨à¤¿à¤µà¤¡',\n",
       " 'à¤•à¥‡à¤²à¥€',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾',\n",
       " 'à¤¤à¥€à¤¨',\n",
       " 'à¤®à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤µ',\n",
       " 'à¤à¤•',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤–à¥‚à¤ª',\n",
       " 'à¤‰à¤‚à¤š',\n",
       " 'à¤…à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤šà¥€',\n",
       " 'à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€',\n",
       " 'à¤‰à¤‚à¤šà¥€',\n",
       " 'à¥§à¥©à¥¯à¥ª',\n",
       " 'à¤®à¥€à¤Ÿà¤°',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤¦à¥à¤°à¥à¤—à¤°à¤¾à¤œ',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤•à¤¾à¤‚à¤•à¥à¤·à¥‡à¤šà¥€',\n",
       " 'à¤‰à¤‚à¤šà¥€',\n",
       " 'à¤¦à¤¾à¤–à¤µà¤¤à¥‹',\n",
       " ',',\n",
       " 'à¤¤à¤°',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥‡',\n",
       " 'à¤°à¤¾à¤¯à¤—à¤¡',\n",
       " 'à¤¹à¤¾',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤•à¤°à¥à¤¤à¥ƒà¤¤à¥à¤µà¤¾à¤šà¤¾',\n",
       " 'à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°',\n",
       " 'à¤¦à¤¾à¤–à¤µà¤¤à¥‹',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤®à¤§à¥à¤¯à¤µà¤°à¥à¤¤à¥€',\n",
       " 'à¤ à¤¿à¤•à¤¾à¤£à¥€',\n",
       " 'à¤‰à¤‚à¤š',\n",
       " 'à¤¡à¥‹à¤‚à¤—à¤°',\n",
       " 'à¤¤à¤¾à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¤¯à¤¾à¤°',\n",
       " 'à¤•à¥‡à¤²à¥‡à¤²à¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤®à¥à¤¹à¤£à¤œà¥‡',\n",
       " 'à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤¨à¥‡',\n",
       " 'à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤µà¤°',\n",
       " 'à¤•à¥‡à¤²à¥‡à¤²à¥€',\n",
       " 'à¤¸à¥à¤µà¤¾à¤°à¥€',\n",
       " 'à¤¹à¥‹à¤¯',\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f8944",
   "metadata": {},
   "source": [
    "## Read file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f19f211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Friends! How are you?\\n', 'Welcome to the world of Python Programming.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"mydata.txt\") as myfile:\n",
    "    print(myfile.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "417aef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \tHow are you?\n",
      "Welcome to the world of \tPython Programming.\n"
     ]
    }
   ],
   "source": [
    "myfile = open(\"mydata.txt\",'r') \n",
    "data = myfile.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984f862",
   "metadata": {},
   "source": [
    "### Space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e002107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " '\\tHow',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " '\\tPython',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer ## only space is used for tokenizing\n",
    "\n",
    "## create an object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "## tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b5b92",
   "metadata": {},
   "source": [
    "### Tab tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00c4c6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! ',\n",
       " 'How are you?\\nWelcome to the world of ',\n",
       " 'Python Programming.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TabTokenizer ## only tab is used for tokenizing\n",
    "\n",
    "## create an object\n",
    "tk = TabTokenizer()\n",
    "\n",
    "## tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4706bb9",
   "metadata": {},
   "source": [
    "### Line tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d06eaa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! \\tHow are you?',\n",
       " 'Welcome to the world of \\tPython Programming.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer ## only \\n is used for tokenizing\n",
    "\n",
    "## create an object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "## tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86167bf",
   "metadata": {},
   "source": [
    "### White space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a950330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer ## all spaces, tabs and \\n are used for tokenizing\n",
    "\n",
    "## create an object\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "## tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76cff4",
   "metadata": {},
   "source": [
    "### MWE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32fb33f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python creator, visiting Pune this week. The \n",
      "developement community is eager to meet Van Rossum\n",
      "['The', 'Van', 'Rossum', 'is', 'Python', 'creator', ',', 'visiting', 'Pune', 'this', 'week', '.', 'The', 'developement', 'community', 'is', 'eager', 'to', 'meet', 'Van', 'Rossum']\n"
     ]
    }
   ],
   "source": [
    "sent1 = '''The Van Rossum is Python creator, visiting Pune this week. The \n",
    "developement community is eager to meet Van Rossum'''\n",
    "\n",
    "print(sent1)\n",
    "\n",
    "print(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0847fc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'developement',\n",
       " 'community',\n",
       " 'is',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer ## always gives output with multiword expression as single token \n",
    "                                        # seperated by a seperator(defaults = '_') for an input from other tokenizer\n",
    "\n",
    "## create an object\n",
    "tk = MWETokenizer(separator=' ')\n",
    "\n",
    "## add mutli word expression\n",
    "tk.add_mwe(('Van', 'Rossum'))\n",
    "\n",
    "## tokenize the data\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f8779",
   "metadata": {},
   "source": [
    "### Tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed0e88ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " ':D',\n",
       " ':|',\n",
       " ':',\n",
       " 'O']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Hello Friends :)! How are you? Welcome to the world of Python Programming. :D :|'\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer ## gives word tokenized with emojis as tokens\n",
    "\n",
    "## create an object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "## tokenize the data\n",
    "tk.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c4ebbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends ðŸ˜€! \tHow are you?ðŸ¤š\n",
      "Welcome ðŸ™ to the worldðŸŒ of \tPython ðŸ’»Programming.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"mydata1.txt\",encoding='utf-8') \n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cef10f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " 'ðŸ˜€',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ¤š',\n",
       " 'Welcome',\n",
       " 'ðŸ™',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldðŸŒ',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'ðŸ’»Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c8be442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " 'ðŸ˜€',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ¤š',\n",
       " 'Welcome',\n",
       " 'ðŸ™',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'ðŸŒ',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'ðŸ’»',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd717c16",
   "metadata": {},
   "source": [
    "### Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2a26d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : \n",
      "Th\n",
      "s\n",
      "s\n",
      "some\n",
      "text\n",
      "w\n",
      "th\n",
      "punctuat\n",
      "on\n",
      ">\n",
      "Let's\n",
      "token\n",
      "ze\n",
      "t\n",
      "Is\n",
      "t\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,:?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens : \")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d085eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mitu.co.in/dataset/student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fbd6771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8b98af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "\n",
    "for i in data.split('\\n'):\n",
    "    sublist = []\n",
    "    for j in i.split('\\t'):\n",
    "        if j.isdigit():\n",
    "            sublist.append(int(j))\n",
    "        elif '.' in(j):\n",
    "            sublist.append(float(j))\n",
    "        else:\n",
    "            sublist.append(j)\n",
    "    list.append(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bc669644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
