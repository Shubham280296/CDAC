{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "679Lmwt3l1Bk"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Vq1MaJI-4uF"
   },
   "source": [
    "# CNN\n",
    "\n",
    "\n",
    "##  Flower Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.helper import fn_plot_tf_hist,fn_plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0QFsplWU-4uH"
   },
   "outputs": [],
   "source": [
    "###----------------------\n",
    "### Some basic parameters\n",
    "###----------------------\n",
    "inpDir = '../..\\Classwork/input'\n",
    "outDir = './output'\n",
    "subDir = 'flower_photos'\n",
    "modelDir = './models'\n",
    "logDir = './logs'\n",
    "altName = 'cnn_base'\n",
    "\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "tf.random.set_seed(RANDOM_STATE) # setting for Tensorflow as well\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "ALPHA = 0.001\n",
    "EPOCHS = 200 # number of cycles to run\n",
    "PATIENCE = 20\n",
    "LR_PATIENCE = 10\n",
    "FACTOR_LR = 0.1\n",
    "BATCH_SIZE = 16 # inline of Training Rows being 60000\n",
    "IMG_HEIGHT = 190\n",
    "IMG_WIDTH = 190\n",
    "\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (15,10),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.coolwarm\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMLb0J4lEnjO"
   },
   "source": [
    "## Basic Hygiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yL2JOjK4-4uI"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xm66whGj_nrn",
    "outputId": "dca7d8d8-6862-4b49-88bc-8eb8013e1dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print (physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qbHlymTEdxG"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "x2uh0xZm-4uJ",
    "outputId": "67ab73a8-e290-4167-aff5-e22e047ac7c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_dir = os.path.join(inpDir, subDir)\\ndata_dir\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "'''\n",
    "data_dir = os.path.join(inpDir, subDir)\n",
    "data_dir\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bcebwqcw-4uK",
    "outputId": "227a647a-4cc7-4efa-a8fe-14915752bd03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'LICENSE.txt', 'roses', 'sunflowers', 'tulips']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dMpgmBvErj2"
   },
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8Shk8Fi-4uL",
    "outputId": "8e654aec-a9f1-442c-abf5-1df91814ab81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_ds =tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, # path the the data directory\n",
    "    validation_split=TEST_SIZE, # what ratio of validation data\n",
    "    subset='training', # purpose\n",
    "    seed=RANDOM_STATE,\n",
    "    image_size=[IMG_HEIGHT, IMG_WIDTH], ## @@@ WHAT!\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "# test data\n",
    "test_ds =tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, # path the the data directory\n",
    "    validation_split=TEST_SIZE, # what ratio of validation data\n",
    "    subset='validation', # purpose\n",
    "    seed=RANDOM_STATE,\n",
    "    image_size=[IMG_HEIGHT, IMG_WIDTH], ## @@@ WHAT!\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FP9Jk9Lf-4uL",
    "outputId": "ebd8967f-64ca-4c85-a602-3e6dfb80c504"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is it picking class names\n",
    "class_names = train_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbxIr7tRAhOj",
    "outputId": "74f0d873-0032-4312-8095-365b62668188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'daisy', 1: 'dandelion', 2: 'roses', 3: 'sunflowers', 4: 'tulips'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {k:v for k,v in enumerate(class_names)}\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ-gvaS-EvN9"
   },
   "source": [
    "## Visualize data in train_ds and test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "klChSP0b-4uM",
    "outputId": "c58fd433-4923-4815-f9ff-694776a12ec7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(15,8))\\n\\nfor images, labels in train_ds.take(1):\\n    for i in range (BATCH_SIZE):\\n        plt.subplot(int(BATCH_SIZE/8), 8, i +1)\\n        plt.grid(False)\\n        plt.imshow(images[i].numpy().astype('uint8'))\\n        plt.title(class_names[labels[i]])\\n        plt.axis('off')\\n    plt.tight_layout()\\nplt.show()\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range (BATCH_SIZE):\n",
    "        plt.subplot(int(BATCH_SIZE/8), 8, i +1)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "b97a_uKg-4uM",
    "outputId": "085058e3-0241-4cf7-812f-2e23440e55fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(15,8))\\n\\nfor images, labels in test_ds.take(1): # get me one batch\\n\\n    for i in range (BATCH_SIZE): # loop over batch\\n\\n        plt.subplot(int(BATCH_SIZE/8), 8, i +1) # access the axis\\n\\n        plt.grid(False) # no to grid\\n\\n        plt.imshow(images[i].numpy().astype('uint8')) # show image convert to numpy and int\\n\\n        plt.title(class_names[labels[i]])\\n\\n        plt.axis('off')\\n\\n    plt.tight_layout()\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "for images, labels in test_ds.take(1): # get me one batch\n",
    "\n",
    "    for i in range (BATCH_SIZE): # loop over batch\n",
    "\n",
    "        plt.subplot(int(BATCH_SIZE/8), 8, i +1) # access the axis\n",
    "\n",
    "        plt.grid(False) # no to grid\n",
    "\n",
    "        plt.imshow(images[i].numpy().astype('uint8')) # show image convert to numpy and int\n",
    "\n",
    "        plt.title(class_names[labels[i]])\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAodZH89EzY2"
   },
   "source": [
    "## To check whether data is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "6C862mXf-4uI",
    "outputId": "cd94477f-3af5-478f-a398-473c25be7c9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef fn_plot_label(tr_ds, ts_ds):\\n\\n    plt.figure(figsize = (15,5)) # instantiate the figure\\n\\n    plt.subplot(1,2,1) # first out of 2\\n\\n    train_labels = tf.concat([lbl for img, lbl in tr_ds], axis = 0).numpy() # get the labels\\n\\n    unique, _, counts = tf.unique_with_counts(train_labels) # get counts\\n\\n    plt.bar(range(len(unique)), counts, align='center', color = 'DarkBlue') # barplot the counts\\n\\n    plt.xticks(range(len(unique)), class_names)\\n\\n    plt.title('Training Set')\\n\\n    plt.subplot(1,2,2)\\n\\n    test_labels = tf.concat([lbl for img, lbl in ts_ds], axis = 0).numpy()\\n\\n    unique, _, counts = tf.unique_with_counts(test_labels)\\n\\n    plt.bar(range(len(unique)), counts, align='center', color = 'Orange')\\n\\n    plt.xticks(range(len(unique)), class_names)\\n\\n    plt.title('Test Set')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def fn_plot_label(tr_ds, ts_ds):\n",
    "\n",
    "    plt.figure(figsize = (15,5)) # instantiate the figure\n",
    "\n",
    "    plt.subplot(1,2,1) # first out of 2\n",
    "\n",
    "    train_labels = tf.concat([lbl for img, lbl in tr_ds], axis = 0).numpy() # get the labels\n",
    "\n",
    "    unique, _, counts = tf.unique_with_counts(train_labels) # get counts\n",
    "\n",
    "    plt.bar(range(len(unique)), counts, align='center', color = 'DarkBlue') # barplot the counts\n",
    "\n",
    "    plt.xticks(range(len(unique)), class_names)\n",
    "\n",
    "    plt.title('Training Set')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "\n",
    "    test_labels = tf.concat([lbl for img, lbl in ts_ds], axis = 0).numpy()\n",
    "\n",
    "    unique, _, counts = tf.unique_with_counts(test_labels)\n",
    "\n",
    "    plt.bar(range(len(unique)), counts, align='center', color = 'Orange')\n",
    "\n",
    "    plt.xticks(range(len(unique)), class_names)\n",
    "\n",
    "    plt.title('Test Set')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "A6hRkrIp-4uN"
   },
   "outputs": [],
   "source": [
    "# fn_plot_label(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0PW-0DJE68-"
   },
   "source": [
    "## Model Building\n",
    "\n",
    "from last conv layer to input layer\n",
    "\n",
    "op size 2*2\n",
    "\n",
    "1. conv layer f = 3, stride (s) =1  ip size = 4*4                        \n",
    "2. maxpool layer f = 2,2, stride (s) =2  ip size = 8*8\n",
    "3. conv layer f = 3, stride (s) =1  ip size = 10*10\n",
    "4. maxpool layer f = 2,2, stride (s) =2  ip size = 20*20\n",
    "5. conv layer f = 3, stride (s) =1  ip size = 22*22\n",
    "6. maxpool layer f = 2,2, stride (s) =2  ip size = 44*44\n",
    "7. conv layer f = 3, stride (s) =1  ip size = 46*46\n",
    "8. maxpool layer f = 2,2, stride (s) =2  ip size = 92*92\n",
    "9. conv layer f = 3, stride (s) =1  ip size = 94*94\n",
    "10. maxpool layer f = 2,2, stride (s) =2  ip size = 188*188\n",
    "11. conv layer f = 3, stride (s) =1  ip size = 190*190  (image size)\n",
    "\n",
    "6 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4izZWt8-4uN",
    "outputId": "0d640a66-7efa-476c-a08d-817a727870b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 190, 3), 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "num_classes = len(class_names)\n",
    "input_shape, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v5SgPGyt-4uN"
   },
   "outputs": [],
   "source": [
    "def build_model (input_shape, num_classes):\n",
    "\n",
    "  krnl_initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  ## increasing dropout rate\n",
    "  drop1 = 0.1\n",
    "  drop2 = 0.1\n",
    "  drop3 = 0.2\n",
    "  drop4 = 0.2\n",
    "  drop5 = 0.3\n",
    "  drop6 = 0.3\n",
    "  drop7 = 0.4\n",
    "  drop8 = 0.4\n",
    "  drop9 = 0.5\n",
    "  drop10 = 0.5\n",
    "\n",
    "  ## preprocessing (scaling)\n",
    "  model.add(tf.keras.layers.Rescaling(1./255.))\n",
    "\n",
    "  ## Augmentation\n",
    "\n",
    "  model.add(tf.keras.layers.RandomRotation((-0.5,0.5), fill_mode = 'nearest', seed=RANDOM_STATE))\n",
    "  model.add(tf.keras.layers.RandomZoom((0.2,0.2), fill_mode = 'nearest', seed=RANDOM_STATE))\n",
    "\n",
    "  ## 1 layer\n",
    "  model.add(tf.keras.layers.Conv2D(32,(3,3),\n",
    "                                   kernel_initializer = krnl_initializer,\n",
    "                                   input_shape =input_shape)) ## output shape expected - 188*188*32\n",
    "\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2))) ## 94*94*32\n",
    "\n",
    "  model.add(tf.keras.layers.Dropout(drop1))\n",
    "\n",
    "  ## 2 layer\n",
    "  model.add(tf.keras.layers.Conv2D(64,3,\n",
    "                                   kernel_initializer = krnl_initializer)) ## output shape expected - 92*92*64\n",
    "\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2))) ## 46*46*64\n",
    "\n",
    "  model.add(tf.keras.layers.Dropout(drop2))\n",
    "\n",
    "  ## 3 layer\n",
    "  model.add(tf.keras.layers.Conv2D(128,(3,3),\n",
    "                                   kernel_initializer = krnl_initializer)) ## output shape expected - 44*44*128\n",
    "\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2))) ## 22*22*128\n",
    "\n",
    "  model.add(tf.keras.layers.Dropout(drop3))\n",
    "\n",
    "  ## 4 layer\n",
    "  model.add(tf.keras.layers.Conv2D(256,(3,3),\n",
    "                                   kernel_initializer = krnl_initializer)) ## output shape expected - 20*20*256\n",
    "\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2))) ## 10*10*256\n",
    "\n",
    "  model.add(tf.keras.layers.Dropout(drop4))\n",
    "\n",
    "  ## 5 layer\n",
    "  model.add(tf.keras.layers.Conv2D(512,(3,3),\n",
    "                                   kernel_initializer = krnl_initializer)) ## output shape expected - 8*8*512\n",
    "\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2))) ## 4*4*512\n",
    "\n",
    "  model.add(tf.keras.layers.Dropout(drop5))\n",
    "\n",
    "  ## 6 layer\n",
    "  model.add(tf.keras.layers.Conv2D(1024,(3,3),\n",
    "                                   kernel_initializer = krnl_initializer)) ## output shape expected - 2*2*1024\n",
    "\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.Dropout(drop6))\n",
    "\n",
    "\n",
    "  ## Head\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(1024, kernel_initializer = krnl_initializer))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "  model.add(tf.keras.layers.Dropout(drop7))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(256, kernel_initializer = krnl_initializer))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "  model.add(tf.keras.layers.Dropout(drop8))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(64, kernel_initializer = krnl_initializer))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "  model.add(tf.keras.layers.Dropout(drop9))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(16, kernel_initializer = krnl_initializer))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.ReLU())\n",
    "  model.add(tf.keras.layers.Dropout(drop10))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(num_classes))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NK2077-9Mxys",
    "outputId": "04b771d0-1e9d-424f-c28e-c0e8dcf2d302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x1b91c212cd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(input_shape, num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9qfbVxxENjtL"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(modelDir, subDir, 'weights_tf_flower')\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=2,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    "    initial_value_threshold=None\n",
    ")\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=PATIENCE,\n",
    "    verbose=2,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=FACTOR_LR,\n",
    "    patience=LR_PATIENCE,\n",
    "    verbose=2,\n",
    "    mode='auto',\n",
    "    min_delta=0.00001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WSAiOVPNAHn"
   },
   "source": [
    "## Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NIxK-zgxNHpw"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=ALPHA)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN_s8DVxOHma",
    "outputId": "a5d9c95b-7232-4a24-845a-70d87e5fa7ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.41727, saving model to ./models\\flower_photos\\weights_tf_flower\n",
      "184/184 - 86s - loss: 1.7246 - accuracy: 0.2858 - val_loss: 2.4173 - val_accuracy: 0.2398 - lr: 0.0010 - 86s/epoch - 469ms/step\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 2.41727 to 1.82140, saving model to ./models\\flower_photos\\weights_tf_flower\n",
      "184/184 - 76s - loss: 1.4290 - accuracy: 0.3873 - val_loss: 1.8214 - val_accuracy: 0.3025 - lr: 0.0010 - 76s/epoch - 411ms/step\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss improved from 1.82140 to 1.27351, saving model to ./models\\flower_photos\\weights_tf_flower\n",
      "184/184 - 76s - loss: 1.3102 - accuracy: 0.4639 - val_loss: 1.2735 - val_accuracy: 0.4755 - lr: 0.0010 - 76s/epoch - 413ms/step\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_ds,\n\u001b[0;32m      2\u001b[0m                     validation_data \u001b[38;5;241m=\u001b[39m test_ds ,\n\u001b[0;32m      3\u001b[0m                     batch_size \u001b[38;5;241m=\u001b[39m BATCH_SIZE,\n\u001b[0;32m      4\u001b[0m                     epochs \u001b[38;5;241m=\u001b[39m EPOCHS, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[model_checkpoint,es_callback,lr_callback])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    validation_data = test_ds ,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    epochs = EPOCHS, verbose=2,\n",
    "                    callbacks=[model_checkpoint,es_callback,lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0W-VX75PPhj"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMUVfsUARJ5H"
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model,'model.png', show_shapes=True, show_dtype=True, dpi=96, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8Rkhy27RHMI"
   },
   "outputs": [],
   "source": [
    "history_df1 = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F08aR88JgC7_"
   },
   "outputs": [],
   "source": [
    "history1 = model.fit(train_ds,\n",
    "                    validation_data = test_ds ,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    epochs = EPOCHS, verbose=2,\n",
    "                    callbacks=[model_checkpoint,es_callback,lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df2 = pd.DataFrame(history1.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.concat((history_df1,history_df2),ignore_index=True)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_tf_hist(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
