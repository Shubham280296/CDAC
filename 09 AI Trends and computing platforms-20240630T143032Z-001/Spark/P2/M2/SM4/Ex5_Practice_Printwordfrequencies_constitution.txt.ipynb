{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67d0ede",
   "metadata": {},
   "source": [
    "# Print word frequencies\n",
    "\n",
    "# Print word frequencies\n",
    "\n",
    "- After combining the values (counts) with the same key (word), you'll print the word frequencies using the `take(N)` action. You could have used the `collect()` action but as a best practice, it is not recommended as `collect()` returns all the elements from your RDD. You'll use `take(N)` instead, to return N elements from your RDD.\n",
    "\n",
    "- What if we want to return the top 10 words? For this first, you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count) and print the top 10 words in descending order.\n",
    "\n",
    "- You already have a `SparkContext` `sc` and `resultRDD` available in your workspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344e9ec",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Print the first 10 words and their frequencies from the `resultRDD`.\n",
    "- Swap the keys and values in the `resultRDD`.\n",
    "- Sort the keys according to descending order.\n",
    "- Print the top 10 most frequent words and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d70cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('We', 2)\n",
      "('the', 662)\n",
      "('People', 2)\n",
      "('of', 493)\n",
      "('United', 85)\n",
      "('States,', 55)\n",
      "('in', 137)\n",
      "('Order', 1)\n",
      "('to', 183)\n",
      "('form', 1)\n",
      "\n",
      " Most frequent word :\n",
      " has 812 counts\n",
      "the has 662 counts\n",
      "of has 493 counts\n",
      "shall has 293 counts\n",
      "and has 256 counts\n",
      "to has 183 counts\n",
      "be has 178 counts\n",
      "or has 157 counts\n",
      "in has 137 counts\n",
      "by has 100 counts\n"
     ]
    }
   ],
   "source": [
    "file_path = \"file:////home/talentum/test-jupyter/P2/M2/SM4/Dataset/constitution.txt\"\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_tup = splitRDD.map(lambda word: (word, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "countRDD = splitRDD_tup.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in countRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "countRDD_swap = countRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "sortedRDD_swap = countRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "print(\"\\n Most frequent word :\")\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in sortedRDD_swap.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5de4558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(812, ''),\n",
       " (662, 'the'),\n",
       " (493, 'of'),\n",
       " (293, 'shall'),\n",
       " (256, 'and'),\n",
       " (183, 'to'),\n",
       " (178, 'be'),\n",
       " (157, 'or'),\n",
       " (137, 'in'),\n",
       " (100, 'by')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"file:////home/talentum/test-jupyter/P2/M2/SM4/Dataset/constitution.txt\"\n",
    "\n",
    "sc.textFile(file_path) \\\n",
    ".flatMap(lambda line: line.split(' ')) \\\n",
    ".map(lambda word: (word, 1)) \\\n",
    ".reduceByKey(lambda x, y: x + y) \\\n",
    ".map(lambda x: (x[1], x[0])) \\\n",
    ".sortByKey(ascending=False).take(10)  ### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88593aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
